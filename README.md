# Kolmogorov-theorem-using-python
# Overview
This project implements and trains a simple feedforward Neural Network (NN) for function approximation and classification tasks. The assignment explores fundamental concepts in neural networks, such as forward propagation, backpropagation, and weight updates using gradient descent.

# Features
Custom Neural Network from Scratch: Implements a simple multi-layer perceptron (MLP) using NumPy for a basic training task.
Kolmogorov Theorem Demonstration: Uses TensorFlow and Keras to approximate the sine function with a neural network.
Backpropagation & Weight Updates: Trains a neural network with gradient-based optimization.
Activation Functions: Uses the sigmoid and ReLU activation functions in different models.
Dependencies
To run this notebook, install the following Python libraries:
pip install numpy tensorflow

# Usage
Open the Jupyter Notebook and execute the cells sequentially.
The first section implements a simple neural network with one hidden layer.
The second section demonstrates Kolmogorov's theorem, showing that any continuous function can be approximated by a neural network.

# Output
The model predicts an output value after training, demonstrating the neural networkâ€™s learning capability.
